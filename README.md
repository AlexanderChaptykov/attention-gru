# Attention-gru
attention-gru implemention from this article https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
# Improvements
- speed improvement of training
- batch training
- bidirectional and layers parameters
- better structure of code
- additional module Attention

